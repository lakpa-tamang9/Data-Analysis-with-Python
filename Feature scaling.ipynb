{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "demographic-struggle",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-nylon",
   "metadata": {},
   "source": [
    "Feature scaling is a method used to normalise the range of independent variables or characteristics of data. In data processing it is also known as data normalisation and is generally performed during the data preprocessing step. We will use three types of sscaling functions available in scikit learn in this examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-rebate",
   "metadata": {},
   "source": [
    "MinMaxScaler: This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-courage",
   "metadata": {},
   "source": [
    "StandardScaler: Standardize features by removing the mean and scaling to unit variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-example",
   "metadata": {},
   "source": [
    "RobustScalar: Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-mouth",
   "metadata": {},
   "source": [
    "Using each of the above scaling techniques, the wine dataset is classified with support vector classifier. The test accuracy of the classification is different for different scaling technique. Support vector classifer details can be found in this link.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "For the specific example of the support vector classifier there is a separate lesson in this repository. Please chcek that out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "separated-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.svm import SVC #we will use support vector classifier to validate the results of these scaling techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "modern-building",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load wine data \n",
    "cancer = load_wine()\n",
    "# Dividing into training data and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "medical-blanket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before scale adjustment features Min value : \n",
      " [1.103e+01 9.000e-01 1.360e+00 1.060e+01 7.800e+01 1.100e+00 4.700e-01\n",
      " 1.300e-01 4.100e-01 1.900e+00 4.800e-01 1.290e+00 2.780e+02]\n",
      "Before scale adjustment features Max value : \n",
      " [1.475e+01 5.510e+00 3.230e+00 2.850e+01 1.620e+02 3.880e+00 5.080e+00\n",
      " 6.300e-01 3.580e+00 1.300e+01 1.710e+00 4.000e+00 1.680e+03]\n"
     ]
    }
   ],
   "source": [
    "print('Before scale adjustment features Min value : \\n {}'. format(X_train.min(axis=0)))\n",
    "print('Before scale adjustment features Max value : \\n {}'. format(X_train.max(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "absent-athletics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scale adjustment features Min value : \n",
      " [-2.37754359 -1.38348819 -3.48106522 -2.57638748 -1.56748077 -1.93064349\n",
      " -1.57745054 -1.93131761 -2.04336071 -1.39330443 -2.04729128 -1.85373978\n",
      " -1.49638849]\n",
      "After scale adjustment features Max value : \n",
      " [2.21491939 3.00435638 2.91172611 2.63173486 4.50881873 2.49597269\n",
      " 2.97849769 2.23451506 3.39953358 3.39449718 3.21157849 1.99124744\n",
      " 3.02707913]\n"
     ]
    }
   ],
   "source": [
    "std_scaler = StandardScaler()\n",
    "X_train_scale = std_scaler.fit_transform(X_train)\n",
    "print('After scale adjustment features Min value : \\n {}'. format(X_train_scale.min(axis=0)))\n",
    "print('After scale adjustment features Max value : \\n {}'. format(X_train_scale.max(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "creative-disorder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scale adjustment features Min value : \n",
      " [-1.5625     -0.63461538 -2.55       -2.06976744 -1.05263158 -1.23364486\n",
      " -0.98837209 -1.3125     -1.68656716 -0.98976109 -1.47058824 -1.12977099\n",
      " -0.9138756 ]\n",
      "After scale adjustment features Max value : \n",
      " [1.34375    2.32051282 2.125      2.09302326 3.36842105 1.36448598\n",
      " 1.69186047 1.8125     3.04477612 2.79863481 2.14705882 0.9389313\n",
      " 2.44019139]\n"
     ]
    }
   ],
   "source": [
    "rob_scaler = RobustScaler()\n",
    "X_train_scale = rob_scaler.fit_transform(X_train)\n",
    "print('After scale adjustment features Min value : \\n {}'. format(X_train_scale.min(axis=0)))\n",
    "print('After scale adjustment features Max value : \\n {}'. format(X_train_scale.max(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "separated-limit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scale adjustment features Min value : \n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "After scale adjustment features Max value : \n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "M_scaler = MinMaxScaler()\n",
    "X_train_scale = M_scaler.fit_transform(X_train)\n",
    "print('After scale adjustment features Min value : \\n {}'. format(X_train_scale.min(axis=0)))\n",
    "print('After scale adjustment features Max value : \\n {}'. format(X_train_scale.max(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "widespread-requirement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scale adjustment features Min value : \n",
      " [8.42814125e-03 9.44379463e-04 1.47299438e-03 7.73940288e-03\n",
      " 6.41465296e-02 1.25511448e-03 6.36567145e-04 1.44840386e-04\n",
      " 5.96123935e-04 2.73340074e-03 6.99051960e-04 1.67493716e-03\n",
      " 9.51811607e-01]\n",
      "After scale adjustment features Max value : \n",
      " [0.0411063  0.01165225 0.00730469 0.06970307 0.29795842 0.00840401\n",
      " 0.0105596  0.00124842 0.00746946 0.02187609 0.00472722 0.01085832\n",
      " 0.9978349 ]\n"
     ]
    }
   ],
   "source": [
    "nor_scaler = Normalizer()\n",
    "X_train_scale = nor_scaler.fit_transform(X_train)\n",
    "print('After scale adjustment features Min value : \\n {}'. format(X_train_scale.min(axis=0)))\n",
    "print('After scale adjustment features Max value : \\n {}'. format(X_train_scale.max(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "individual-afghanistan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy : 0.733\n"
     ]
    }
   ],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "print('test accuracy : %.3f' %(svc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "lucky-elevation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled test accuracy : 0.578\n"
     ]
    }
   ],
   "source": [
    "# You can see that the performance is better.\n",
    "X_test_scale = nor_scaler.fit_transform(X_test)\n",
    "svc.fit(X_train_scale, y_train)\n",
    "print('Scaled test accuracy : %.3f' %(svc.score(X_test_scale, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "alien-status",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scale adjustment features Min value : \n",
      " [-2.37754359 -1.38348819 -3.48106522 -2.57638748 -1.56748077 -1.93064349\n",
      " -1.57745054 -1.93131761 -2.04336071 -1.39330443 -2.04729128 -1.85373978\n",
      " -1.49638849]\n",
      "After scale adjustment features Max value : \n",
      " [-2.37754359 -1.38348819 -3.48106522 -2.57638748 -1.56748077 -1.93064349\n",
      " -1.57745054 -1.93131761 -2.04336071 -1.39330443 -2.04729128 -1.85373978\n",
      " -1.49638849]\n"
     ]
    }
   ],
   "source": [
    "std_scaler = StandardScaler()\n",
    "X_train_scale = std_scaler.fit_transform(X_train)\n",
    "print('After scale adjustment features Min value : \\n {}'. format(X_train_scale.min(axis=0)))\n",
    "print('After scale adjustment features Max value : \\n {}'. format(X_train_scale.min(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "general-harvest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled test accuracy : 0.978\n"
     ]
    }
   ],
   "source": [
    "# You can see that the performance is better.\n",
    "X_test_scale = std_scaler.fit_transform(X_test)\n",
    "svc.fit(X_train_scale, y_train)\n",
    "print('Scaled test accuracy : %.3f' %(svc.score(X_test_scale, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "immediate-short",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scale adjustment features Min value : \n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "After scale adjustment features Max value : \n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "M_scaler = MinMaxScaler()\n",
    "X_train_scale = M_scaler.fit_transform(X_train)\n",
    "print('After scale adjustment features Min value : \\n {}'. format(X_train_scale.min(axis=0)))\n",
    "print('After scale adjustment features Max value : \\n {}'. format(X_train_scale.max(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "essential-absorption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled test accuracy : 0.933\n"
     ]
    }
   ],
   "source": [
    "# You can see that the performance is better.\n",
    "X_test_scale = M_scaler.fit_transform(X_test)\n",
    "svc.fit(X_train_scale, y_train)\n",
    "print('Scaled test accuracy : %.3f' %(svc.score(X_test_scale, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
